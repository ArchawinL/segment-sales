{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2551ddeb",
   "metadata": {},
   "source": [
    "# Segmentation and Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91eabdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "# Set JAVA_HOME environment variable before importing PySpark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e7f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0.17\n"
     ]
    }
   ],
   "source": [
    "online_retail = fetch_ucirepo(id=352)\n",
    "online_retail_full = online_retail.data.original\n",
    "spark = SparkSession.builder.appName(\"ecommerce\").getOrCreate()\n",
    "print(spark.sparkContext._gateway.jvm.System.getProperty(\"java.version\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73086420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:20:09 WARN TaskSetManager: Stage 70 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/archawin/segment-sales/venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/archawin/segment-sales/venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "    outfile.flush()\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "online_retail_full.to_csv(\"online_retail.csv\", index=False)\n",
    "online_retail_spark = spark.read.csv(\"online_retail.csv\", header=True, inferSchema=True)\n",
    "online_retail_spark = spark.createDataFrame(online_retail.data.original)\n",
    "online_retail_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a18dfe",
   "metadata": {},
   "source": [
    "**Metadata and null checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d178f9",
   "metadata": {},
   "source": [
    "In spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4064991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:59:09 WARN TaskSetManager: Stage 109 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541909\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "None\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:59:09 WARN TaskSetManager: Stage 112 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(online_retail_spark.count())\n",
    "\n",
    "print(online_retail_spark.printSchema())\n",
    "print(online_retail_spark.rdd.getNumPartitions())\n",
    "online_retail_spark.filter(F.col(\"CustomerID\").isNull()).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadf4fe",
   "metadata": {},
   "source": [
    "In SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffbc053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:22:59 WARN TaskSetManager: Stage 91 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_retail_spark.createOrReplaceTempView(\"retail\")\n",
    "spark.sql(\"SELECT COUNT (*) FROM retail WHERE CustomerID IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a15f2f",
   "metadata": {},
   "source": [
    "## Let us explore some basic information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a369011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:51:39 WARN TaskSetManager: Stage 103 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|       Country|      TotalRevenue|\n",
      "+--------------+------------------+\n",
      "|United Kingdom| 8187806.364000115|\n",
      "|   Netherlands|         284661.54|\n",
      "|          EIRE|263276.81999999995|\n",
      "|       Germany|221698.20999999993|\n",
      "|        France|197403.90000000002|\n",
      "+--------------+------------------+\n",
      "\n",
      "SQL Output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 14:51:40 WARN TaskSetManager: Stage 106 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|       Country|      TotalRevenue|\n",
      "+--------------+------------------+\n",
      "|United Kingdom| 8187806.364000115|\n",
      "|   Netherlands|         284661.54|\n",
      "|          EIRE|263276.81999999995|\n",
      "|       Germany|221698.20999999993|\n",
      "|        France|197403.90000000002|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top countries by revenue\n",
    "df_with_revenue = online_retail_spark.withColumn(\"Revenue\", F.col(\"Quantity\") * F.col(\"UnitPrice\"))\n",
    "top_countries = df_with_revenue.groupBy(\"Country\") \\\n",
    "    .agg(F.sum(\"Revenue\").alias(\"TotalRevenue\")) \\\n",
    "    .orderBy(F.desc(\"TotalRevenue\")) \\\n",
    "    .limit(5)\n",
    "\n",
    "top_countries.show()\n",
    "# Try with SQL, should match\n",
    "print(\"SQL Output\")\n",
    "spark.sql(\"SELECT Country, SUM(Quantity * UnitPrice) as TotalRevenue FROM retail GROUP BY Country ORDER BY TotalRevenue DESC LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7b66f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 15:02:00 WARN TaskSetManager: Stage 124 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/23 15:02:01 WARN TaskSetManager: Stage 127 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|         Description|TotalQuantity|\n",
      "+--------------------+-------------+\n",
      "|WORLD WAR 2 GLIDE...|        53847|\n",
      "|JUMBO BAG RED RET...|        47363|\n",
      "|ASSORTED COLOUR B...|        36381|\n",
      "|      POPCORN HOLDER|        36334|\n",
      "|PACK OF 72 RETROS...|        36039|\n",
      "|WHITE HANGING HEA...|        35317|\n",
      "|  RABBIT NIGHT LIGHT|        30680|\n",
      "|MINI PAINT SET VI...|        26437|\n",
      "|PACK OF 12 LONDON...|        26315|\n",
      "|PACK OF 60 PINK P...|        24753|\n",
      "+--------------------+-------------+\n",
      "\n",
      "SQL Output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:====================>                                 (12 + 20) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|         Description|TotalQuantity|\n",
      "+--------------------+-------------+\n",
      "|WORLD WAR 2 GLIDE...|        53847|\n",
      "|JUMBO BAG RED RET...|        47363|\n",
      "|ASSORTED COLOUR B...|        36381|\n",
      "|      POPCORN HOLDER|        36334|\n",
      "|PACK OF 72 RETROS...|        36039|\n",
      "|WHITE HANGING HEA...|        35317|\n",
      "|  RABBIT NIGHT LIGHT|        30680|\n",
      "|MINI PAINT SET VI...|        26437|\n",
      "|PACK OF 12 LONDON...|        26315|\n",
      "|PACK OF 60 PINK P...|        24753|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Top best-selling products by total quantity sold\n",
    "top_products = online_retail_spark.groupBy(\"Description\") \\\n",
    "    .agg(F.sum(\"Quantity\").alias(\"TotalQuantity\")).orderBy(F.desc(\"TotalQuantity\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_products.show()\n",
    "\n",
    "print(\"SQL Output\")\n",
    "spark.sql(\"SELECT Description, sum(Quantity) as TotalQuantity FROM retail GROUP BY Description ORDER BY TotalQuantity DESC LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=10, orderBy=[TotalQuantity#758L DESC NULLS LAST], output=[Description#436,TotalQuantity#758L])\n",
      "   +- HashAggregate(keys=[Description#436], functions=[sum(Quantity#437L)])\n",
      "      +- Exchange hashpartitioning(Description#436, 200), ENSURE_REQUIREMENTS, [plan_id=1771]\n",
      "         +- HashAggregate(keys=[Description#436], functions=[partial_sum(Quantity#437L)])\n",
      "            +- Project [Description#436, Quantity#437L]\n",
      "               +- Scan ExistingRDD[InvoiceNo#434,StockCode#435,Description#436,Quantity#437L,InvoiceDate#438,UnitPrice#439,CustomerID#440,Country#441]\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=10, orderBy=[TotalQuantity#790L DESC NULLS LAST], output=[Description#436,TotalQuantity#790L])\n",
      "   +- HashAggregate(keys=[Description#436], functions=[sum(Quantity#437L)])\n",
      "      +- Exchange hashpartitioning(Description#436, 200), ENSURE_REQUIREMENTS, [plan_id=1792]\n",
      "         +- HashAggregate(keys=[Description#436], functions=[partial_sum(Quantity#437L)])\n",
      "            +- Project [Description#436, Quantity#437L]\n",
      "               +- Scan ExistingRDD[InvoiceNo#434,StockCode#435,Description#436,Quantity#437L,InvoiceDate#438,UnitPrice#439,CustomerID#440,Country#441]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the execution plans. They should be the same as Spark is API abstraction of SQL\n",
    "df_plan = top_products.explain()\n",
    "print(\"\\n---\\n\")\n",
    "sql_plan = spark.sql(\"SELECT Description, sum(Quantity) as TotalQuantity FROM retail GROUP BY Description ORDER BY TotalQuantity DESC LIMIT 10\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "473da0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 16:52:43 WARN TaskSetManager: Stage 176 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 16:52:43 WARN TaskSetManager: Stage 179 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "530104"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = online_retail_spark.filter((online_retail_spark['Quantity'] > 0) & (online_retail_spark['UnitPrice'] > 0))\n",
    "print(filtered_df.count())\n",
    "\n",
    "\n",
    "# SQL\n",
    "spark.sql(\"SELECT * FROM retail WHERE (Quantity > 0 AND UnitPrice > 0)\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7bee8cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|        InvoiceDate|Hour|\n",
      "+-------------------+----+\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:26:00|   8|\n",
      "|2010-12-01 08:28:00|   8|\n",
      "|2010-12-01 08:28:00|   8|\n",
      "|2010-12-01 08:34:00|   8|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/23 17:12:33 WARN TaskSetManager: Stage 210 contains a task of very large size (1013 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/archawin/segment-sales/venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/archawin/segment-sales/venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "    outfile.flush()\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = filtered_df.withColumn('InvoiceDate', F.to_timestamp(F.col('InvoiceDate'), \"M/d/yyyy H:mm\"))\n",
    "cleaned_df\n",
    "\n",
    "cleaned_df.createOrReplaceTempView(\"retail_clean\")\n",
    "\n",
    "\n",
    "#SQL Query to check\n",
    "spark.sql(\"SELECT InvoiceDate, hour(InvoiceDate) as Hour FROM retail_clean LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7c4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c247484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
